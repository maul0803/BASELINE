{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:32.738221Z",
     "start_time": "2024-09-24T18:25:32.719884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:52:02.482620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-25 13:52:02.496690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 13:52:02.512986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 13:52:02.517907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 13:52:02.530319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 13:52:29.299893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Start')\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import numpy as np  # Linear algebra\n",
    "import json  # To read json\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW  # AdamW instead of Adam because it's better for SQuAD\n",
    "from collections import Counter\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter  # For TensorBoard\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I used is the dev-v2 dataset. It is a json file. I didn't know how to read this file so I used a code from kaggle:  \n",
    "https://www.kaggle.com/code/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:32.783172Z",
     "start_time": "2024-09-24T18:25:32.770098Z"
    }
   },
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    m = pd.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    m['context'] = idx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:33.220382Z",
     "start_time": "2024-09-24T18:25:32.817318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "processing...\n",
      "shape of the dataframe is (11873, 4)\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'context', 'answers'],\n",
       "    num_rows: 11873\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = 'dev-v2.0.json'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "dataset = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curent dataset can't be used for training. It must be tokenized before. I didn't know how to do it so I have used a code from CHATGPT that I have modidified.  \n",
    "To simplify the dataset, only the first answer for each question is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:41.033992Z",
     "start_time": "2024-09-24T18:25:33.254708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/ulc/cursos/curso341/mypython/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d4f1f940104b18b9c36334cd14b61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/8687672/ipykernel_3104668/2412163777.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs[column] = torch.tensor(inputs[column])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#CHATGPT\n",
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    answers = examples['answers']\n",
    "    \n",
    "    inputs = tokenizer(questions, contexts, max_length=384, truncation=True, padding='max_length', return_offsets_mapping=True)\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i in range(len(questions)):\n",
    "        if not answers[i]:  # Si pas de réponse\n",
    "            start_positions.append(0)  # Valeur par défaut\n",
    "            end_positions.append(0)    # Valeur par défaut\n",
    "            continue\n",
    "        \n",
    "        # Only the firs answer is used\n",
    "        first_answer = answers[i][0]['text']\n",
    "        first_answer_start = answers[i][0]['answer_start']\n",
    "        \n",
    "        offsets = inputs['offset_mapping'][i]\n",
    "\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= first_answer_start < end:\n",
    "                start_token = idx\n",
    "            if start < first_answer_start + len(first_answer) <= end:\n",
    "                end_token = idx\n",
    "                break\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            start_positions.append(start_token)\n",
    "            end_positions.append(end_token)\n",
    "        else:\n",
    "            start_positions.append(-1)  # Default value\n",
    "            end_positions.append(-1)    # Default value\n",
    "\n",
    "\n",
    "    inputs.pop('offset_mapping') #offset_mapping is not necessary to train the model\n",
    "    \n",
    "    # Converting everything into tensors \n",
    "    inputs.update({\n",
    "        'start_positions': torch.tensor(start_positions),\n",
    "        'end_positions': torch.tensor(end_positions)\n",
    "    })\n",
    "    for column in inputs.keys():\n",
    "        inputs[column] = torch.tensor(inputs[column])\n",
    "\n",
    "    \n",
    "\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'question', 'context', 'answers'])\n",
    "tokenized_dataset = tokenized_dataset.select(range(1600))#Decrease the size of the dataset to have a longer training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in the tokenized dataset should be a tensor otherwise the training won't be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids <class 'list'>\n",
      "token_type_ids <class 'list'>\n",
      "attention_mask <class 'list'>\n",
      "start_positions <class 'list'>\n",
      "end_positions <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for column in ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']:\n",
    "    print(column, type(tokenized_dataset[column]))  # Devrait être torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that dataset.map didn't change the type of the dataset.  \n",
    "To change the format I have used set_format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:41.080829Z",
     "start_time": "2024-09-24T18:25:41.068299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids <class 'torch.Tensor'>\n",
      "token_type_ids <class 'torch.Tensor'>\n",
      "attention_mask <class 'torch.Tensor'>\n",
      "start_positions <class 'torch.Tensor'>\n",
      "end_positions <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for column in ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']:\n",
    "    print(column, type(tokenized_dataset[column]))  # Devrait être torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T18:25:41.126742Z",
     "start_time": "2024-09-24T18:25:41.113518Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=tokenized_dataset, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=tokenized_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "optim = AdamW(model.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model choosen for this task is bert base cased.  \n",
    "The optimizer is AdamW which is similar the the usual Adam optimizer but with weight decay. It seems that this kind of optimizer is better for transformer models.  \n",
    "The learning rate at the begning of the training is 5e-5 which is the usual learning rate for transformsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default loss for this model is the binary crossentropy loss.  \n",
    "To have something else from the loss to display on tensorboard I used the f1_score and a exact_match score.  \n",
    "The f1_score wasn't imported from scikit_learn because to use it I had to use numpy arrays.  \n",
    "However, to get numpy arrays it seems that it is necessary to transfer my results from the GPU to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHATGPT for the implementation of metrics and in the training\n",
    "# Function to calculate F1-score\n",
    "def f1_score(pred_toks, true_toks):\n",
    "    common = Counter(pred_toks) & Counter(true_toks)  # Find common tokens\n",
    "    num_common = sum(common.values())  # Count how many tokens are in common\n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    precision = num_common / len(pred_toks)\n",
    "    recall = num_common / len(true_toks)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Function to calculate Exact Match (EM)\n",
    "def exact_match_score(pred, true):\n",
    "    return int(pred == true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "checkpoint_dir = \"./checkpoints\"  # Directory where checkpoints will be saved\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230637aa2b6f48439855b047ad2ff23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.3796\n",
      "Exact Match at epoch 1: 0.4938\n",
      "F1 Score at epoch 1: 0.4894\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081e2a3b592542a7ace2543567a9e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 2: 1.5862\n",
      "Exact Match at epoch 2: 0.5159\n",
      "F1 Score at epoch 2: 0.4582\n",
      "Checkpoint saved: ./checkpoints/checkpoint_epoch_2.pt\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cd2fe6a06c4416894ddcc75ed4cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 3: 1.0825\n",
      "Exact Match at epoch 3: 0.6284\n",
      "F1 Score at epoch 3: 0.5650\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee68bf5cccd9465d892a81d220ee60ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 4: 0.7239\n",
      "Exact Match at epoch 4: 0.7163\n",
      "F1 Score at epoch 4: 0.6623\n",
      "Checkpoint saved: ./checkpoints/checkpoint_epoch_4.pt\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6c383d2f9348f3a57fc894cbd6a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 5: 0.5365\n",
      "Exact Match at epoch 5: 0.8003\n",
      "F1 Score at epoch 5: 0.7563\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e46c1a1a52a4599b7a4e003e32c64b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 6: 0.3884\n",
      "Exact Match at epoch 6: 0.8512\n",
      "F1 Score at epoch 6: 0.8253\n",
      "Checkpoint saved: ./checkpoints/checkpoint_epoch_6.pt\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53564536527941bd88a1d9e8e5ea1a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 7: 0.3603\n",
      "Exact Match at epoch 7: 0.8762\n",
      "F1 Score at epoch 7: 0.8505\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0703e80aae2c4c70a6ebb0ae8737d305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 8: 0.2260\n",
      "Exact Match at epoch 8: 0.9216\n",
      "F1 Score at epoch 8: 0.9060\n",
      "Checkpoint saved: ./checkpoints/checkpoint_epoch_8.pt\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c08d9457b34896b67022f011a53961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 9: 0.2117\n",
      "Exact Match at epoch 9: 0.9184\n",
      "F1 Score at epoch 9: 0.9034\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929b2228866046bfa0e80c56269740ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 10: 0.1691\n",
      "Exact Match at epoch 10: 0.9341\n",
      "F1 Score at epoch 10: 0.9203\n",
      "Checkpoint saved: ./checkpoints/checkpoint_epoch_10.pt\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard setup: log directory\n",
    "log_dir = \"./logs_exclusive\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Create a checkpoint every 2 epochs\n",
    "save_checkpoint_every = 2\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()  # Set model to training mode, inside the loop if I want to add the evaluation with a validation dataset.\n",
    "    \n",
    "    total_loss = 0  # Accumulate total loss\n",
    "    total_em = 0  # Total Exact Match\n",
    "    total_f1 = 0  # Total F1 score\n",
    "    num_questions = 0  # Number of questions processed (for the metrics)\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "################################################################################################\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        # Loss\n",
    "        loss = outputs.loss  # Default loss is binary crossentropy\n",
    "        total_loss += loss.item()\n",
    "        # Backward pass and weight update\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "################################################################################################\n",
    "        start_preds = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_preds = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        #CHATGPT\n",
    "        # Metrics computation for each batch\n",
    "        batch_size = input_ids.size(0)\n",
    "        for i in range(batch_size):\n",
    "            # Exact Match for start and end positions\n",
    "            em_start = exact_match_score(start_preds[i].item(), start_positions[i].item())\n",
    "            em_end = exact_match_score(end_preds[i].item(), end_positions[i].item())\n",
    "            total_em += (em_start + em_end) / 2  # Average Exact Match for start and end\n",
    "\n",
    "            # F1 Score for the predicted vs true tokens\n",
    "            pred_tokens = input_ids[i][start_preds[i]:end_preds[i]+1].tolist()\n",
    "            true_tokens = input_ids[i][start_positions[i]:end_positions[i]+1].tolist()\n",
    "            total_f1 += f1_score(pred_tokens, true_tokens)\n",
    "\n",
    "        num_questions += batch_size\n",
    "\n",
    "    # Calculate average loss and metrics for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    avg_em = total_em / num_questions  # Normalize Exact Match\n",
    "    avg_f1 = total_f1 / num_questions  # Normalize F1 score\n",
    "################################################################################################\n",
    "    # Print out metrics for the epoch\n",
    "    print(f\"Loss at epoch {epoch+1}: {avg_loss:.4f}\")\n",
    "    print(f\"Exact Match at epoch {epoch+1}: {avg_em:.4f}\")\n",
    "    print(f\"F1 Score at epoch {epoch+1}: {avg_f1:.4f}\")\n",
    "    \n",
    "    # Log average loss and metrics for the epoch to TensorBoard\n",
    "    writer.add_scalar(\"Loss/epoch_avg\", avg_loss, epoch)\n",
    "    writer.add_scalar(\"Metrics/Exact_Match\", avg_em, epoch)\n",
    "    writer.add_scalar(\"Metrics/F1_Score\", avg_f1, epoch)\n",
    "\n",
    "    # Save a checkpoint every 'save_checkpoint_every' epochs\n",
    "    if (epoch + 1) % save_checkpoint_every == 0:\n",
    "        save_checkpoint(model, optim, epoch, avg_loss, checkpoint_dir)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "print('Training completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
